<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Spark 使用手册俗话说，工欲善其事，必先利其器，Spark无疑是现在大数据计算框架中的一颗明星，就像他的名字Spark~  虽然Spark出道许久，现在市场也有更为先进及时的大数据处理框架 Flink ，但是现在从Spark开始认真学习大数据依然是不错的选择，因为其底层思想即承接了他的前辈MR的优势，又紧跟步伐在 SparkStreaming 中支持流批一体数据处理，在某些不是很极端的情况下，">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark 使用手册">
<meta property="og:url" content="http://example.com/2022/08/23/Spark-%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/index.html">
<meta property="og:site_name" content="kanaikee-BigData">
<meta property="og:description" content="Spark 使用手册俗话说，工欲善其事，必先利其器，Spark无疑是现在大数据计算框架中的一颗明星，就像他的名字Spark~  虽然Spark出道许久，现在市场也有更为先进及时的大数据处理框架 Flink ，但是现在从Spark开始认真学习大数据依然是不错的选择，因为其底层思想即承接了他的前辈MR的优势，又紧跟步伐在 SparkStreaming 中支持流批一体数据处理，在某些不是很极端的情况下，">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-08-23T13:38:31.000Z">
<meta property="article:modified_time" content="2022-08-23T13:39:46.832Z">
<meta property="article:author" content="kanaikee">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2022/08/23/Spark-%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Spark 使用手册 | kanaikee-BigData</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">kanaikee-BigData</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/23/Spark-%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kanaikee">
      <meta itemprop="description" content="春风不语，即问本心">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="kanaikee-BigData">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark 使用手册
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-08-23 21:38:31 / 修改时间：21:39:46" itemprop="dateCreated datePublished" datetime="2022-08-23T21:38:31+08:00">2022-08-23</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Spark-使用手册"><a href="#Spark-使用手册" class="headerlink" title="Spark 使用手册"></a>Spark 使用手册</h1><p>俗话说，工欲善其事，必先利其器，Spark无疑是现在大数据计算框架中的一颗明星，就像他的名字Spark~ </p>
<p>虽然Spark出道许久，现在市场也有更为先进及时的大数据处理框架 <strong>Flink</strong> ，但是现在从Spark开始认真学习大数据依然是不错的选择，因为其底层思想即承接了他的前辈MR的优势，又紧跟步伐在 <strong>SparkStreaming</strong> 中支持流批一体数据处理，在某些不是很极端的情况下，依然是大数据计算引擎中的中流砥柱</p>
<p>本文将采用Scala作为Spark学习的唯一代码</p>
<h2 id="Spark三种连接方式"><a href="#Spark三种连接方式" class="headerlink" title="Spark三种连接方式"></a>Spark三种连接方式</h2><p>对我来说，首先要学会用，才能进一步了解底层原理，所以我们先粗浅说一下如何连接Spark到各种数据源~</p>
<h3 id="1、Spark-core"><a href="#1、Spark-core" class="headerlink" title="1、Spark core"></a>1、Spark core</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">.setAppName(<span class="string">&quot;name&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="comment">//方式一</span></span><br><span class="line"><span class="keyword">val</span> value: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;src/main/resources/pvuvdata&quot;</span>)</span><br><span class="line"><span class="comment">//方式二</span></span><br><span class="line"><span class="keyword">val</span> value: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.parallelize(arr)</span><br><span class="line"><span class="comment">//写出</span></span><br><span class="line">value.foreach(println)</span><br><span class="line"><span class="comment">//关闭</span></span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure>


<h3 id="2、spark-SQL"><a href="#2、spark-SQL" class="headerlink" title="2、spark SQL"></a>2、spark SQL</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkSession: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">.appName(<span class="string">&quot;hello01SparkSql&quot;</span>)</span><br><span class="line">.getOrCreate()</span><br><span class="line"><span class="comment">//方式一、文本读入</span></span><br><span class="line"><span class="keyword">val</span> value: <span class="type">RDD</span>[<span class="type">String</span>] = sparkSession</span><br><span class="line">.sparkContext</span><br><span class="line">.textFile(<span class="string">&quot;src/main/resources/emp.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//方式二 指定格式 读入 </span></span><br><span class="line"><span class="keyword">val</span> dFt1: <span class="type">DataFrame</span> = sparkSession</span><br><span class="line">.read</span><br><span class="line">.format(<span class="string">&quot;格式&quot;</span>).load(<span class="string">&quot;Path&quot;</span>)</span><br><span class="line"><span class="comment">//方式三，规定格式读入 </span></span><br><span class="line"> <span class="keyword">val</span> dFt2: <span class="type">Dataset</span>[<span class="type">Row</span>] = sparkSession</span><br><span class="line">.read</span><br><span class="line">.格式(<span class="string">&quot;Path&quot;</span>)</span><br><span class="line">.as(<span class="string">&quot;Class类&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//写出到其他地方</span></span><br><span class="line">df.show()</span><br><span class="line">df.write().mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).format(<span class="string">&quot;格式&quot;</span>).save(<span class="string">&quot;path&quot;</span>);</span><br><span class="line">df.write().mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).格式(<span class="string">&quot;path&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//关闭</span></span><br><span class="line">sparkSession.stop()</span><br></pre></td></tr></table></figure>



<h4 id="2-0-DSL查询与SQL查询"><a href="#2-0-DSL查询与SQL查询" class="headerlink" title="2.0. DSL查询与SQL查询"></a>2.0. DSL查询与SQL查询</h4><h5 id="2-0-1-DSL查询"><a href="#2-0-1-DSL查询" class="headerlink" title="2.0.1. DSL查询"></a>2.0.1. DSL查询</h5><blockquote>
<ul>
<li>DSL就是用算子进行查询<ul>
<li>通过样式类 case class 方式转化为dataFrame</li>
<li>通过结构化类型 structType 方式转化为dataFrame</li>
</ul>
</li>
</ul>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//开始转换,</span></span><br><span class="line"><span class="keyword">val</span> dataFrame = sparkSession.createDataFrame(empRDD, empStructType)</span><br><span class="line"></span><br><span class="line"><span class="comment">//将RDD转成DataFrame</span></span><br><span class="line"><span class="keyword">import</span> sparkSession.implicits._</span><br><span class="line"><span class="keyword">val</span> <span class="type">Dataset</span>: <span class="type">Dataset</span>[<span class="type">Emp</span>] = emps.toDS()</span><br></pre></td></tr></table></figure>

<h5 id="2-0-2-SQL查询"><a href="#2-0-2-SQL查询" class="headerlink" title="2.0.2. SQL查询"></a>2.0.2. SQL查询</h5><p>createOrReplaceTempView(“表名”) 创建一个零时表，通过SQL方式查询</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkSession: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&quot;StructType&quot;</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> value: <span class="type">RDD</span>[<span class="type">String</span>] = sparkSession.sparkContext.textFile(<span class="string">&quot;src/main/resources/emp.txt&quot;</span>)</span><br><span class="line">    <span class="comment">//进行转换(每行都构建一个Emp)</span></span><br><span class="line">    <span class="keyword">val</span> empRDD: <span class="type">RDD</span>[<span class="type">Row</span>] = value.map(_.split(<span class="string">&quot;,&quot;</span>)).map(e =&gt; <span class="type">Row</span>(e(<span class="number">0</span>).toInt, e(<span class="number">1</span>), e(<span class="number">2</span>), e(<span class="number">3</span>).toInt, e(<span class="number">4</span>), e(<span class="number">5</span>).toDouble, e(<span class="number">6</span>).toDouble, e(<span class="number">7</span>).toInt))</span><br><span class="line">    <span class="comment">//转成dataFrame</span></span><br><span class="line">    <span class="keyword">val</span> dfr: <span class="type">DataFrame</span> = sparkSession.createDataFrame(empRDD,empSt)</span><br><span class="line">    <span class="comment">//创建临时表，使用SQL语法查询</span></span><br><span class="line">    dfr.createOrReplaceTempView(<span class="string">&quot;t_emp&quot;</span>)</span><br><span class="line">    sparkSession.sql(<span class="string">&quot;select * from t_emp&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="2-1-普通文本"><a href="#2-1-普通文本" class="headerlink" title="2.1. 普通文本"></a>2.1. 普通文本</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//读取</span></span><br><span class="line">sparkSession.sparkContext.textFile(<span class="string">&quot;src/main/resources/emp.txt&quot;</span>)</span><br><span class="line"><span class="comment">//写出</span></span><br><span class="line">empSal.saveAsTextFile(<span class="string">&quot;src/main/resources/t_emp_sal.txt&quot;</span>)</span><br></pre></td></tr></table></figure>



<h4 id="2-2-json"><a href="#2-2-json" class="headerlink" title="2.2. json"></a>2.2. json</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkSession: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&quot;hello01_sql&quot;</span>).getOrCreate()</span><br><span class="line">    <span class="comment">//方式一：读取</span></span><br><span class="line">    <span class="keyword">val</span> dFt1: <span class="type">DataFrame</span> = sparkSession.read.json(<span class="string">&quot;src/main/resources/t_emp1.txt&quot;</span>).as(<span class="string">&quot;Emp&quot;</span>)</span><br><span class="line">    <span class="comment">//方式二：读取</span></span><br><span class="line">    <span class="comment">//val dFt2: DataFrame = sparkSession.read.format(&quot;json&quot;).load(&quot;src/main/resources/t_emp.txt&quot;)</span></span><br><span class="line"><span class="comment">//写出</span></span><br><span class="line"><span class="comment">//dFt1.write.mode(SaveMode.Overwrite).format(&quot;json&quot;).save(&quot;src/main/resources/t_emp1.txt&quot;)</span></span><br><span class="line"><span class="comment">//写出 方式二</span></span><br><span class="line">	dFt1.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).json(<span class="string">&quot;src/main/resources/t_emp.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">	dFt1.show()</span><br><span class="line"><span class="comment">//dFt2.show()</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-3-parquet"><a href="#2-3-parquet" class="headerlink" title="2.3. parquet"></a>2.3. parquet</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	<span class="keyword">val</span> sparkSession: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&quot;sql04_parquet&quot;</span>).getOrCreate()</span><br><span class="line">    <span class="comment">//读入 方式一</span></span><br><span class="line">	<span class="comment">//val dFt1: DataFrame = sparkSession.read.format(&quot;parquet&quot;).load(&quot;src/main/resources/t_emp.txt&quot;)</span></span><br><span class="line">    <span class="comment">//读入 方式二</span></span><br><span class="line">    <span class="keyword">val</span> dFt2: <span class="type">Dataset</span>[<span class="type">Row</span>] = sparkSession.read.parquet(<span class="string">&quot;src/main/resources/t_emp.txt&quot;</span>).as(<span class="string">&quot;Emp&quot;</span>)</span><br><span class="line">	<span class="comment">//写出 方式一</span></span><br><span class="line"><span class="comment">//dFt2.write.mode(SaveMode.Overwrite).format(&quot;parquet&quot;).save(&quot;src/main/resources/t_emp1.txt&quot;)</span></span><br><span class="line">	<span class="comment">//写出 方式二</span></span><br><span class="line">	dFt2.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).parquet(<span class="string">&quot;src/main/resources/t_emp1.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">	dFt2.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="2-4-jdbc"><a href="#2-4-jdbc" class="headerlink" title="2.4. jdbc"></a>2.4. jdbc</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建SQL环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkSession: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&quot;sql04_parquet&quot;</span>).getOrCreate()</span><br><span class="line">    <span class="comment">//数据库参数</span></span><br><span class="line">    <span class="keyword">val</span> map = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]()</span><br><span class="line">    map.put(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://localhost:3306/java46?useSSL=false&amp;serverTimezone=UTC&amp;characterEncoding=utf8&quot;</span>)</span><br><span class="line">    map.put(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.cj.jdbc.Driver&quot;</span>)</span><br><span class="line">    map.put(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">    map.put(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">    map.put(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;emp&quot;</span>)</span><br><span class="line">    <span class="comment">//读取JDBC数据 方式一</span></span><br><span class="line">	<span class="keyword">val</span> dFt1: <span class="type">DataFrame</span> = sparkSession.read.format(<span class="string">&quot;jdbc&quot;</span>).options(map).load()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//数据库参数</span></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.setProperty(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.cj.jdbc.Driver&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">    <span class="comment">//读取JDBC数据 方式二</span></span><br><span class="line">    <span class="keyword">val</span> dFt2: <span class="type">DataFrame</span> = sparkSession.read</span><br><span class="line">      .jdbc(<span class="string">&quot;jdbc:mysql://localhost:3306/java46?useSSL=false&amp;serverTimezone=UTC&amp;characterEncoding=utf8&quot;</span>, <span class="string">&quot;emp&quot;</span>, properties)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//写入JDBC数据 方式一</span></span><br><span class="line">    dFt2.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">      .jdbc(<span class="string">&quot;jdbc:mysql://localhost:3306/java46?serverTimezone=UTC&amp;characterEncoding=utf8&amp;useSSL=false&quot;</span>,<span class="string">&quot;t_emp&quot;</span>,properties)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//写入JDBC数据 方式二</span></span><br><span class="line">    map.put(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;t_emp3&quot;</span>)</span><br><span class="line">    dFt1.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">      .options(map).save(<span class="string">&quot;jdbc:mysql://localhost:3306/java46?serverTimezone=UTC&amp;characterEncoding=utf8&amp;useSSL=false&quot;</span>)</span><br><span class="line"></span><br><span class="line">    dFt1.show()</span><br><span class="line">    dFt2.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="2-5-hive"><a href="#2-5-hive" class="headerlink" title="2.5. hive"></a>2.5. hive</h4><p>拷贝配置文件,从linux上拷贝hadoop、hive配置</p>
<blockquote>
<p>hdfs-site.xml</p>
<p>core-site.xml</p>
<p>hive-site.xml</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yjxxtimport org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line">	<span class="class"><span class="keyword">object</span> <span class="title">HelloSourceHive</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">		<span class="comment">//搭建环境</span></span><br><span class="line">		<span class="keyword">val</span> spark =<span class="type">SparkSession</span>.builder().master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&quot;HelloSourceHive&quot;</span>).enableHiveSupport().getOrCreate()</span><br><span class="line">		<span class="comment">//操作数据</span></span><br><span class="line">		spark.sql(<span class="string">&quot;use yjx&quot;</span>)</span><br><span class="line">		<span class="keyword">val</span> dataFrame = spark.sql(<span class="string">&quot;select * from t_user&quot;</span>)</span><br><span class="line">		dataFrame.show()</span><br><span class="line">		<span class="comment">//关闭</span></span><br><span class="line">		sparkSession.stop()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3 id="3、spark-streaming"><a href="#3、spark-streaming" class="headerlink" title="3、spark streaming"></a>3、spark streaming</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">//搭建环境</span></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;App&quot;</span>)</span><br><span class="line">  <span class="comment">//设置参数</span></span><br><span class="line">  <span class="keyword">val</span> streamingContext = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, streaming.<span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">  <span class="comment">//获取数据</span></span><br><span class="line">  <span class="keyword">val</span> value: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = streamingContext.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">  <span class="comment">//操作数据</span></span><br><span class="line">  <span class="keyword">val</span> dStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = value.flatMap(_.split(<span class="string">&quot;\\s&quot;</span>)).map((_, <span class="number">1</span>)).reduceByKey((x, y) =&gt; x + y)</span><br><span class="line">  <span class="comment">//打印数据</span></span><br><span class="line">  dStream.print()</span><br><span class="line">    </span><br><span class="line">  <span class="comment">//开启服务</span></span><br><span class="line">  streamingContext.start()</span><br><span class="line">  <span class="comment">//等待停止</span></span><br><span class="line">  streamingContext.awaitTermination()</span><br><span class="line">  <span class="comment">//关闭服务</span></span><br><span class="line">  streamingContext.stop(<span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>







<h4 id="3-2-streaming-kafka"><a href="#3-2-streaming-kafka" class="headerlink" title="3.2. streaming+kafka"></a>3.2. streaming+kafka</h4><p>首先要启动zkserver<br>再启动kafka</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//搭建环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    .setAppName(<span class="string">&quot;stream05_kafka&quot;</span>)</span><br><span class="line">    .set(<span class="string">&quot;spark.streaming.stopGracefullyOnShutdown&quot;</span>,<span class="string">&quot;true&quot;</span>)</span><br><span class="line">    <span class="comment">//设置streaming封装间隔</span></span><br><span class="line">    <span class="keyword">val</span> streamingContext = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, streaming.<span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">//kafka配置</span></span><br><span class="line">    <span class="keyword">val</span> kafkaPar: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="string">&quot;node01:9092,node02:9092,node03:9092&quot;</span>,</span><br><span class="line">      <span class="string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;yjx_bigdata&quot;</span>,</span><br><span class="line">      <span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;earliest&quot;</span>,</span><br><span class="line">      <span class="string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="literal">true</span>: lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//创建主题</span></span><br><span class="line">    <span class="keyword">val</span> topics: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="string">&quot;userlog&quot;</span>)</span><br><span class="line">    <span class="comment">//创建kafka</span></span><br><span class="line">    <span class="keyword">val</span> kfDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]]</span><br><span class="line">        = <span class="type">KafkaUtils</span>.createDirectStream(streamingContext, <span class="type">PreferConsistent</span>, <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaPar))</span><br><span class="line">    <span class="comment">//编辑数据</span></span><br><span class="line">    <span class="keyword">val</span> result: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = kfDStream.map(_.value()).flatMap(_.split(<span class="string">&quot; &quot;</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">    <span class="comment">//打印数据</span></span><br><span class="line">    result.print()</span><br><span class="line">    <span class="comment">//result.saveAsHadoopFiles(&quot;yjx&quot;, &quot;txt&quot;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启streaming</span></span><br><span class="line">    streamingContext.start()</span><br><span class="line">    streamingContext.awaitTermination()</span><br><span class="line">    <span class="comment">//关闭streaming</span></span><br><span class="line">    streamingContext.stop(<span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="Spark-应用分析"><a href="#Spark-应用分析" class="headerlink" title="Spark 应用分析"></a>Spark 应用分析</h2><p>Spark 作为一个流批一体的大数据处理框架，他的作用无外乎三个：</p>
<blockquote>
<ol>
<li>大数据处理 ETL (Data → Data)</li>
<li>大数据分析 BI (Data → Dashboard)</li>
<li>机器学习 AI (Data → Model)</li>
</ol>
</blockquote>
<h3 id="何为ETL？"><a href="#何为ETL？" class="headerlink" title="何为ETL？"></a>何为ETL？</h3><p>这里引用 <a target="_blank" rel="noopener" href="https://www.ibm.com/cloud/learn/etl">IBM</a> 的解释</p>
<blockquote>
<h1 id="ETL（提取、转换、加载）"><a href="#ETL（提取、转换、加载）" class="headerlink" title="ETL（提取、转换、加载）"></a>ETL（提取、转换、加载）</h1><p>ETL 是一个从多个来源提取、转换和加载数据到数据仓库或其他统一数据存储库的过程。</p>
<p>ETL 代表提取、转换和加载，是一种数据集成过程，它将来自多个数据源的数据组合到一个单一的、一致的数据存储中，然后再加载到 <a target="_blank" rel="noopener" href="https://www.ibm.com/cloud/learn/data-warehouse">数据仓库</a> 或其他目标系统中。</p>
<p>随着数据库在 1970 年代的普及，ETL 被引入作为集成和加载数据以进行计算和分析的过程，最终成为数据仓库项目处理数据的主要方法。</p>
<p>ETL 为<strong>数据分析</strong>和<strong>机器学习</strong>工作流提供了基础。通过一系列业务规则，ETL 以解决特定商业智能需求的方式清理和组织数据，例如<strong>月度报告</strong>，但它还可以处理更高级的分析，从而改善后端流程或最终用户体验。组织通常使用 ETL 来： </p>
<ul>
<li><strong>从遗留系统中提取数据</strong></li>
<li><strong>清理数据以提高数据质量并建立一致性</strong></li>
<li><strong>将数据加载到目标数据库</strong></li>
</ul>
</blockquote>
<p>因为大数据时代数据充满了不确定性，极高的冗余度，数据的错误等等，用普通的方式很难在<strong>成吨</strong>数据中提取出我们真正想要的数据，所以 ETL 由此诞生。</p>
<p>ETL的流程也如其名：<strong>Extract, Transform, Load</strong></p>
<blockquote>
<h1 id="提炼"><a href="#提炼" class="headerlink" title="提炼"></a>提炼</h1><p>在数据提取过程中，原始数据会从源位置复制或导出到暂存区。数据管理团队可以从各种数据源中提取数据，这些数据源可以是结构化的或非结构化的。这些来源包括但不限于：</p>
<ul>
<li>SQL 或 <a target="_blank" rel="noopener" href="https://www.ibm.com/cloud/learn/nosql-databases">NoSQL</a> 服务器</li>
<li>CRM和ERP系统</li>
<li>平面文件</li>
<li>电子邮件</li>
<li>网页</li>
</ul>
<h1 id="转换"><a href="#转换" class="headerlink" title="转换"></a>转换</h1><p>在暂存区，对原始数据进行数据处理。在这里，数据被转换和整合以用于其预期的分析用例。此阶段可能涉及以下任务：</p>
<ul>
<li>过滤、清理、重复数据删除、验证和验证数据。</li>
<li>根据原始数据执行计算、转换或汇总。这可以包括更改行和列标题以保持一致性、转换货币或其他计量单位、编辑文本字符串等。</li>
<li>进行审计以确保数据质量和合规性</li>
<li>删除、加密或保护由行业或政府监管机构管理的数据</li>
<li>将数据格式化为表或连接表以匹配目标数据仓库的架构。</li>
</ul>
<h1 id="加载"><a href="#加载" class="headerlink" title="加载"></a>加载</h1><p>在这最后一步中，转换后的数据从暂存区移动到目标数据仓库。通常，这涉及到所有数据的初始加载，然后是定期加载增量数据更改，并且不太常见的是完全刷新以擦除和替换仓库中的数据。对于大多数使用 ETL 的组织而言，该过程是自动化的、定义明确的、连续的和批处理驱动的。通常，ETL 发生在源系统和数据仓库上的流量最低的非工作时间。</p>
</blockquote>
<p>ETL 工具</p>
<blockquote>
<h2 id="Datastage"><a href="#Datastage" class="headerlink" title="Datastage"></a>Datastage</h2><p>IBM公司的商业软件，最专业的ETL工具，但同时价格不菲，适合大规模的ETL应用。</p>
<h2 id="Informatica"><a href="#Informatica" class="headerlink" title="Informatica"></a>Informatica</h2><p>商业软件，相当专业的ETL工具。价格上比Datastage便宜一点，也适合大规模的ETL应用。</p>
<h2 id="Kettle"><a href="#Kettle" class="headerlink" title="Kettle"></a>Kettle</h2><p>免费，最著名的开源产品，是用纯java编写的ETL工具，只需要JVM环境即可部署，可跨平台，扩展性好。</p>
</blockquote>
<p>三种 ETL 工具比较</p>
<blockquote>
<p><strong>1、操作</strong></p>
<p>这三种ETL工具都是属于比较简单易用的，主要看开发人员对于工具的熟练程度。</p>
<p>Informatica有四个开发管理组件，开发的时候我们需要打开其中三个进行开发，Informatica没有ctrl+z的功能，如果对job作了改变之后，想要撤销，返回到改变前是不可能的。相比Kettle跟Datastage在测试调试的时候不太方便。Datastage全部的操作在同一个界面中，不用切换界面，能够看到数据的来源，整个job的情况，在找bug的时候会比Informatica方便。</p>
<p>Kettle介于两者之间。</p>
<p><strong>2、部署</strong></p>
<p>Kettle只需要JVM环境，Informatica需要服务器和客户端安装，而Datastage的部署比较耗费时间，有一点难度。</p>
<p><strong>3、数据处理的速度</strong></p>
<p>大数据量下Informatica与Datastage的处理速度是比较快的，比较稳定。Kettle的处理速度相比之下稍慢。</p>
<p><strong>4、服务</strong></p>
<p>Informatica与Datastage有很好的商业化的技术支持，而Kettle则没有。商业软件的售后服务上会比免费的开源软件好很多。</p>
<p><strong>5、风险</strong></p>
<p>风险与成本成反比，也与技术能力成正比。</p>
<p><strong>6、扩展</strong></p>
<p>Kettle的扩展性无疑是最好，因为是开源代码，可以自己开发拓展它的功能，而Informatica和Datastage由于是商业软件，基本上没有。</p>
<p><strong>7、Job的监控</strong></p>
<p>三者都有监控和日志工具。</p>
<p>在数据的监控上，个人觉得Datastage的实时监控做的更加好，可以直观看到数据抽取的情况，运行到哪一个控件上。这对于调优来说，我们可以更快的定位到处理速度太慢的控件并进行处理，而informatica也有相应的功能，但是并不直观，需要通过两个界面的对比才可以定位到处理速度缓慢的控件。有时候还需要通过一些方法去查找。</p>
<p><strong>8、网上的技术文档</strong></p>
<p>Datastage &lt; Informatica &lt; kettle,相对来说，Datastage跟Informatica在遇到问题去网上找到解决方法的概率比较低，kettle则比较多。</p>
</blockquote>
<h3 id="何为BI？"><a href="#何为BI？" class="headerlink" title="何为BI？"></a>何为BI？</h3><p>定义 维基百科</p>
<blockquote>
<p><strong>商业智能（Business Intelligence, BI）</strong>，指用现代<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%B3%87%E6%96%99%E5%80%89%E5%84%B2">数据仓库</a>技术、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%B7%9A%E4%B8%8A%E5%88%86%E6%9E%90%E8%99%95%E7%90%86">在线分析处理</a>技术、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98">数据挖掘</a>和资料展现技术进行资料分析以实现<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%95%86%E6%A5%AD">商业</a>价值</p>
<p>商业智能的概念经由Howard Dresner（1989年）的通俗化而被人们广泛了解。当时将商业智能定义为一类由<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%B3%87%E6%96%99%E5%80%89%E5%84%B2">数据仓库</a>（或<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%B3%87%E8%A8%8A%E5%B8%82%E9%9B%86">信息市集</a>）、查询报表、资料分析、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%B3%87%E6%96%99%E6%8E%A2%E5%8B%98">数据挖掘</a>、资料备份和恢复等部分组成的、以帮助企业决策为目的技术及其应用。</p>
<p>目前，商业智能通常被理解为将<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BC%81%E6%A5%AD">企业</a>中现有的资料转化为<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%9F%A5%E8%AD%98">知识</a>，帮助企业做出明智的业务经营决策的工具。这里所谈的数据包括来自企业业务系统的订单、库存、交易账目、客户和供应商资料及来自企业所处行业和竞争对手的资料，以及来自企业所处的其他外部环境中的各种资料。而商业智能能够辅助的业务经营决策既可以是作业层的，也可以是管理层和策略层的决策。</p>
<p>商业智能可以定义为一系列商业活动行为的资料收集与信息转化作业，透过持续性的过程，搭配技术进行测量、管理与监测，能够容易分析、综合营运及策略的定量化信息应用，即时且交互的对企业的关键性的衡量指针进行评估，进而发觉企业面临的潜在问题或机会，促使用户能够运用大量而完整的信息，进行交叉分析并了解其中趋势，协助企业制订出最佳的策略主题与策略目标的一种决策支持的工具。</p>
</blockquote>
<p>BI技术运用</p>
<blockquote>
<ul>
<li>数据分析</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://powerbi.microsoft.com/">Power BI</a> （<a target="_blank" rel="noopener" href="https://web.archive.org/web/20201217080138/https://powerbi.microsoft.com/">页面存档备份</a>，存于<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%92%E8%81%94%E7%BD%91%E6%A1%A3%E6%A1%88%E9%A6%86">互联网档案馆</a>）, <a target="_blank" rel="noopener" href="https://www.tableau.com/">Tableau</a> （<a target="_blank" rel="noopener" href="https://web.archive.org/web/20210109223556/https://www.tableau.com/">页面存档备份</a>，存于<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%92%E8%81%94%E7%BD%91%E6%A1%A3%E6%A1%88%E9%A6%86">互联网档案馆</a>）, <a target="_blank" rel="noopener" href="http://www.lcnet.com.tw/">Smart eVision</a>（<a target="_blank" rel="noopener" href="https://web.archive.org/web/20191128165820/http://www.lcnet.com.tw/">页面存档备份</a>，存于<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%92%E8%81%94%E7%BD%91%E6%A1%A3%E6%A1%88%E9%A6%86">互联网档案馆</a>）</p>
<ul>
<li>数据仓储</li>
</ul>
<p><a target="_blank" rel="noopener" href="http://www.asterdata.com/">Teradata Aster Data</a> （<a target="_blank" rel="noopener" href="https://web.archive.org/web/20200919010718/http://www.asterdata.com/">页面存档备份</a>，存于<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%92%E8%81%94%E7%BD%91%E6%A1%A3%E6%A1%88%E9%A6%86">互联网档案馆</a>）, <a target="_blank" rel="noopener" href="https://greenplum.org/">EMC GreenPlum</a> （<a target="_blank" rel="noopener" href="https://web.archive.org/web/20201111224512/https://greenplum.org/">页面存档备份</a>，存于<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%92%E8%81%94%E7%BD%91%E6%A1%A3%E6%A1%88%E9%A6%86">互联网档案馆</a>）, <a target="_blank" rel="noopener" href="https://www.vertica.com/overview/">HP Vertica</a> （<a target="_blank" rel="noopener" href="https://web.archive.org/web/20201128175310/https://www.vertica.com/overview/">页面存档备份</a>，存于<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%92%E8%81%94%E7%BD%91%E6%A1%A3%E6%A1%88%E9%A6%86">互联网档案馆</a>） }}</p>
<ul>
<li>数据集市</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://www.qlik.com/">QlikView</a>（<a target="_blank" rel="noopener" href="https://web.archive.org/web/20170430232324/https://www.qlik.com/">页面存档备份</a>，存于<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%92%E8%81%94%E7%BD%91%E6%A1%A3%E6%A1%88%E9%A6%86">互联网档案馆</a>）, <a target="_blank" rel="noopener" href="https://datafocus.ai/">DataFocus</a>（<a target="_blank" rel="noopener" href="https://web.archive.org/web/20180811180104/https://datafocus.ai/">页面存档备份</a>，存于<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%92%E8%81%94%E7%BD%91%E6%A1%A3%E6%A1%88%E9%A6%86">互联网档案馆</a>）, Tableau, Yonghong Data Mart</p>
<ul>
<li>数据采集</li>
</ul>
<p>Kettle</p>
<ul>
<li>OLAP（分布式数据分析）</li>
<li>决策支持系统</li>
</ul>
</blockquote>
<p>企业导入BI的优点</p>
<blockquote>
<ul>
<li>随机查询动态报表</li>
<li>掌握指针管理</li>
<li>随时在线分析处理</li>
<li>可视化之企业仪表版</li>
<li>协助预测规划</li>
</ul>
</blockquote>
<p>导入BI的目的</p>
<blockquote>
<ul>
<li><p>促进企业决策流程（Facilitate the Business Decision-Making Process）：BIS增进企业的信息集成与信息分析的能力，汇总公司内、外部的资料，集成成有效的决策信息，让企业经理人大幅增进决策效率与改善决策质量。</p>
</li>
<li><p>降低整体营运成本（Power the Bottom Line）：BIS改善企业的信息获取能力，大幅降低IT人员撰写程序、Power user制作报表的时间与人力成本，而弹性的模块设计接口，完全不需撰写程序的特色也让日后的维护成本大幅降低。</p>
</li>
<li><p>协同组织目标与行动（Achieve a Fully Coordinated Organization）：BIS加强企业的信息传播能力，消除信息需求者与IT人员之间的认知差距，并可让更多人获得更有意义的信息。全面改善企业之体质，使组织内的每个人目标一致、齐心协力。</p>
</li>
</ul>
</blockquote>
<h3 id="何为-AI-？"><a href="#何为-AI-？" class="headerlink" title="何为 AI ？"></a>何为 AI ？</h3><p>定义 维基百科</p>
<blockquote>
<p><strong>人工智能</strong>（英语：artificial intelligence，缩写为<strong>AI</strong>）亦称<strong>智械</strong>、<strong>机器智能</strong>，指由人制造出来的机器所表现出来的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%99%BA%E6%85%A7">智能</a>。通常人工智能是指通过普通计算机程序来呈现人类智能的技术。该词也指出研究这样的智能系统是否能够实现，以及如何实现。同时，通过<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E9%86%AB%E5%AD%B8">医学</a>、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%A5%9E%E7%B6%93%E7%A7%91%E5%AD%B8">神经科学</a>、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AD%A6">机器人学</a>及<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%B5%B1%E8%A8%88%E5%AD%B8">统计学</a>等的进步，常态预测则认为人类的很多职业也逐渐被其取代。[<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD#cite_note-1">1]</a>[<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD#cite_note-2">2]</a></p>
<p>人工智能于一般教材中的定义领域是“智能主体（intelligent agent）的研究与设计”[<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD#cite_note-Definition_of_AI-3">3]</a>，智能主体指一个可以观察周遭环境并作出行动以达致目标的系统[<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD#cite_note-Intelligent_agents-4">4]</a>。<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%BA%A6%E7%BF%B0%C2%B7%E9%BA%A6%E5%8D%A1%E9%94%A1">约翰·麦卡锡</a>于1955年的定义是[<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD#cite_note-Coining_of_the_term_AI-5">5]</a>“制造智能机器的科学与工程”[<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD#cite_note-McCarthy's_definition_of_AI-6">6]</a>。<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%AE%89%E5%BE%B7%E7%83%88%E4%BA%9A%E6%96%AF%C2%B7%E5%8D%A1%E6%99%AE%E5%85%B0">安德烈亚斯·卡普兰</a>（Andreas Kaplan）和迈克尔·海恩莱因（Michael Haenlein）将人工智能定义为“系统正确解释外部数据，从这些数据中学习，并利用这些知识通过灵活适应实现特定目标和任务的能力”。[<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD#cite_note-7">7]</a> 人工智能可以定义为模仿人类与人类思维相关的认知功能的机器或计算机，如学习和解决问题。人工智能是计算机科学的一个分支，它感知其环境并采取行动，最大限度地提高其成功机会。此外，人工智能能够从过去的经验中学习，做出合理的决策，并快速回应。因此，人工智能研究人员的科学目标是通过构建具有象征意义的推理或推理的计算机程序来理解智能。人工智能的四个主要组成部分是：</p>
<ul>
<li>专家系统：作为专家处理正在审查的情况，并产生预期或预期的绩效。</li>
<li>启发式问题解决：包括评估小范围的解决方案，并可能涉及一些猜测，以找到接近最佳的解决方案。</li>
<li>自然语言处理：在自然语言中实现人机之间的交流。</li>
<li>计算机视觉：自动生成识别形状和功能的能力 [<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD#cite_note-8">8]</a>。</li>
</ul>
<p>人工智能的研究是高度技术性和专业的，各分支领域都是深入且各不相通的，因而涉及范围极广[<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD#cite_note-Fragmentation_of_AI-9">9]</a>。人工智能的研究可以分为几个技术问题。其分支领域主要集中在解决具体问题，其中之一是，如何使用各种不同的工具完成特定的应用程序。</p>
<p>AI的核心问题包括建构能够跟人类似甚至超卓的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%8E%A8%E7%90%86">推理</a>、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%9F%A5%E8%AF%86">知识</a>、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%A6%8F%E5%8A%83">规划</a>、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%AD%A6%E4%B9%A0">学习</a>、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%A4%E6%B5%81">交流</a>、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5">感知</a>、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/w/index.php?title=%E7%A7%BB%E7%89%A9&action=edit&redlink=1">移物</a>、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/w/index.php?title=%E4%BD%BF%E7%94%A8%E5%B7%A5%E5%85%B7&action=edit&redlink=1">使用工具</a>和操控<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%9C%BA%E6%A2%B0">机械</a>的能力等[<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD#cite_note-Problems_of_AI-10">10]</a>。人工智能目前仍然是该领域的长远目标[<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD#cite_note-General_intelligence-11">11]</a>。目前弱人工智能已经有初步成果，甚至在一些影像识别、语言分析、棋类游戏等等单方面的能力达到了超越人类的水平，而且人工智能的通用性代表着，能解决上述的问题的是一样的AI程序，无须重新开发算法就可以直接使用现有的AI完成任务，与人类的处理能力相同，但达到具备思考能力的统合强人工智能还需要时间研究，比较流行的方法包括统计方法，计算智能和传统意义的AI。目前有大量的工具应用了人工智能，其中包括搜索和数学优化、逻辑推演。而基于<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BB%BF%E7%94%9F%E5%AD%B8">仿生学</a>、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%AA%8D%E7%9F%A5%E5%BF%83%E7%90%86%E5%AD%B8">认知心理学</a>，以及基于概率论和经济学的算法等等也在逐步探索当中。</p>
</blockquote>
<p>在此三个领域中，看似都跟Spark没有什么关系，唯一一个看似有关系的ETL也有较为成熟的ETL工具，可视化的操作，上手比较快，但是对于数据量上升导致性能出问题，可优化的空间就不是很大了，毕竟底层人家都已经帮你封装好了，对于大量的数据，ETL工具就显得力不从心了，因为，所以还需要Spark对数据处理</p>
<p>在BI领域同样的，也会有此类问题，只是问题较少，因为BI分析大多为企业内部展示，可以预先规划时间并处理好数据</p>
<p>在AI领域，Spark 的ML框架对于大数据学习有着很强的增补，N卡GPU 也可加速 Apache Spark 3 数据科学管道，加快数据处理和模型训练速度，同时大幅降低基础设施成本。ML框架也集成了较为先进的机器学习算法：<strong>逻辑回归，朴素贝叶斯，线性回归，SVM，决策树，LDA，矩阵分解</strong> 等</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/03/20/hello-world/" rel="prev" title="Hello World">
      <i class="fa fa-chevron-left"></i> Hello World
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C"><span class="nav-number">1.</span> <span class="nav-text">Spark 使用手册</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark%E4%B8%89%E7%A7%8D%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F"><span class="nav-number">1.1.</span> <span class="nav-text">Spark三种连接方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81Spark-core"><span class="nav-number">1.1.1.</span> <span class="nav-text">1、Spark core</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81spark-SQL"><span class="nav-number">1.1.2.</span> <span class="nav-text">2、spark SQL</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-0-DSL%E6%9F%A5%E8%AF%A2%E4%B8%8ESQL%E6%9F%A5%E8%AF%A2"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">2.0. DSL查询与SQL查询</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-0-1-DSL%E6%9F%A5%E8%AF%A2"><span class="nav-number">1.1.2.1.1.</span> <span class="nav-text">2.0.1. DSL查询</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-0-2-SQL%E6%9F%A5%E8%AF%A2"><span class="nav-number">1.1.2.1.2.</span> <span class="nav-text">2.0.2. SQL查询</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-%E6%99%AE%E9%80%9A%E6%96%87%E6%9C%AC"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">2.1. 普通文本</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-json"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">2.2. json</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-parquet"><span class="nav-number">1.1.2.4.</span> <span class="nav-text">2.3. parquet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-jdbc"><span class="nav-number">1.1.2.5.</span> <span class="nav-text">2.4. jdbc</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-hive"><span class="nav-number">1.1.2.6.</span> <span class="nav-text">2.5. hive</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81spark-streaming"><span class="nav-number">1.1.3.</span> <span class="nav-text">3、spark streaming</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-streaming-kafka"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">3.2. streaming+kafka</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-%E5%BA%94%E7%94%A8%E5%88%86%E6%9E%90"><span class="nav-number">1.2.</span> <span class="nav-text">Spark 应用分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%95%E4%B8%BAETL%EF%BC%9F"><span class="nav-number">1.2.1.</span> <span class="nav-text">何为ETL？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ETL%EF%BC%88%E6%8F%90%E5%8F%96%E3%80%81%E8%BD%AC%E6%8D%A2%E3%80%81%E5%8A%A0%E8%BD%BD%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">ETL（提取、转换、加载）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8F%90%E7%82%BC"><span class="nav-number">3.</span> <span class="nav-text">提炼</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BD%AC%E6%8D%A2"><span class="nav-number">4.</span> <span class="nav-text">转换</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD"><span class="nav-number">5.</span> <span class="nav-text">加载</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Datastage"><span class="nav-number">5.1.</span> <span class="nav-text">Datastage</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Informatica"><span class="nav-number">5.2.</span> <span class="nav-text">Informatica</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kettle"><span class="nav-number">5.3.</span> <span class="nav-text">Kettle</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%95%E4%B8%BABI%EF%BC%9F"><span class="nav-number">5.3.1.</span> <span class="nav-text">何为BI？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%95%E4%B8%BA-AI-%EF%BC%9F"><span class="nav-number">5.3.2.</span> <span class="nav-text">何为 AI ？</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">kanaikee</p>
  <div class="site-description" itemprop="description">春风不语，即问本心</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2022-03 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">kanaikee</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
